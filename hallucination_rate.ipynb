{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e52d912-3e4e-43fd-8638-6e721a56ecb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai import generative_models\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14756cfd-4134-43bc-83e0-72fbcb9e42c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: åˆå§‹åŒ– Vertex AI é¡¹ç›®ä¸åŒºåŸŸ\n",
    "aiplatform.init(\n",
    "    project=\"vertex-ai-test-465220\",\n",
    "    location=\"us-central1\"  # Gemini æ”¯æŒåŒºåŸŸ\n",
    ")\n",
    "# Step 5: åˆå§‹åŒ– Gemini æ¨¡å‹ï¼ˆå¼€å¯å·¥å…·æ”¯æŒï¼‰\n",
    "generation_model  = GenerativeModel(\n",
    "    model_name=\"gemini-2.5-pro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dea517a6-07e4-4030-a7bf-013e454722a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== 1. å‡†å¤‡é—®é¢˜ä¸æ­£ç¡®ç­”æ¡ˆ ==========\n",
    "questions = [\n",
    "    \"Who developed the Gemini model?\",\n",
    "    \"What is the capital of Canada?\",\n",
    "    \"Does Gemini support multi-modal input?\",\n",
    "    \"What is the largest animal on Earth?\",\n",
    "    \"Is Saturn the closest planet to the Sun?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Gemini was developed by Google DeepMind.\",\n",
    "    \"Ottawa is the capital of Canada.\",\n",
    "    \"Yes, Gemini supports multi-modal input including text and images.\",\n",
    "    \"The blue whale is the largest animal on Earth.\",\n",
    "    \"No, Mercury is the closest planet to the Sun.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7d74ce3-4d10-49a5-892c-4446060e4c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== 2. ç”Ÿæˆå›ç­” ==========\n",
    "generated_answers = []\n",
    "for q in questions:\n",
    "    prompt = f\"{q} Please answer in one concise sentence.\"\n",
    "    response = generation_model.generate_content(prompt)\n",
    "    generated_answers.append(response.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0d520de-dfcf-4b65-b139-c7b7a15e9218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Gemini model was developed by Google DeepMind in collaboration with other teams across Google.',\n",
       " 'The capital of Canada is Ottawa.',\n",
       " 'Yes, Gemini is a natively multimodal model, capable of understanding and combining different types of information like text, images, audio, and video.',\n",
       " 'The largest animal on Earth is the blue whale.',\n",
       " 'No, Mercury is the closest planet to the Sun.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "843bf450-6e75-4d55-b4a4-b530c31d1627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== 3. ä½¿ç”¨æ¨¡å‹åˆ¤æ–­æ˜¯å¦å¹»è§‰ ==========\n",
    "def check_consistency(generated: str, ground_truth: str) -> bool:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Gemini æ¨¡å‹åˆ¤æ–­ç”Ÿæˆå›ç­”æ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´ã€‚\n",
    "    è¿”å› True è¡¨ç¤ºä¸€è‡´ï¼ˆæ— å¹»è§‰ï¼‰ï¼ŒFalse è¡¨ç¤ºå¹»è§‰ã€‚\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a factual correctness evaluator.\n",
    "\n",
    "Compare the following generated answer with the ground truth.\n",
    "\n",
    "Generated Answer:\n",
    "{generated}\n",
    "\n",
    "Ground Truth:\n",
    "{ground_truth}\n",
    "\n",
    "Does the generated answer contradict or hallucinate compared to the ground truth? \n",
    "Answer \"Yes\" if it contains hallucination or contradiction, otherwise answer \"No\".\n",
    "\n",
    "Your answer (Yes/No):\"\"\"\n",
    "\n",
    "    judge_response = generation_model.generate_content(prompt)\n",
    "    result = judge_response.text.strip().lower()\n",
    "    return \"no\" in result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abb65af4-35d5-440f-ba24-22e5d72da6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Consistent answer for Q1: Who developed the Gemini model?\n",
      "\n",
      "âœ… Consistent answer for Q2: What is the capital of Canada?\n",
      "\n",
      "âŒ Hallucination in Q3: Does Gemini support multi-modal input?\n",
      "Generated: Yes, Gemini is a natively multimodal model, capable of understanding and combining different types of information like text, images, audio, and video.\n",
      "Ground Truth: Yes, Gemini supports multi-modal input including text and images.\n",
      "\n",
      "âœ… Consistent answer for Q4: What is the largest animal on Earth?\n",
      "\n",
      "âœ… Consistent answer for Q5: Is Saturn the closest planet to the Sun?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========== 4. æ‰§è¡Œåˆ¤å®š + ç»Ÿè®¡å¹»è§‰ ==========\n",
    "hallucinated = []\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    is_consistent = check_consistency(generated_answers[i], ground_truths[i])\n",
    "    if not is_consistent:\n",
    "        hallucinated.append(i)\n",
    "        print(f\"\\nâŒ Hallucination in Q{i+1}: {questions[i]}\")\n",
    "        print(f\"Generated: {generated_answers[i]}\")\n",
    "        print(f\"Ground Truth: {ground_truths[i]}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… Consistent answer for Q{i+1}: {questions[i]}\")\n",
    "\n",
    "hallucinated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e53e3b6-fceb-40bf-9f3e-6219559df490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Hallucination Rate: 20.00% (1/5)\n"
     ]
    }
   ],
   "source": [
    "# ========== 5. è¾“å‡ºå¹»è§‰ç‡ ==========\n",
    "hallucination_rate = len(hallucinated) / len(questions)\n",
    "print(f\"\\nğŸ“Š Hallucination Rate: {hallucination_rate:.2%} ({len(hallucinated)}/{len(questions)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a715a34-f1c6-43cb-990f-bd96a673083b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
